{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Count Vectorizer, Tfidf Vectorizer</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "def getDocumentList(fileName):\n",
    "    data = pd.read_csv(fileName, sep=\";\", encoding=\"utf8\")\n",
    "    documents = data['icerik'].tolist()\n",
    "    doc_lst = [] \n",
    "    for doc in documents:\n",
    "        doc_lst.append(doc.split())\n",
    "    return doc_lst\n",
    "\n",
    "def train_word2vec(p_documents, p_num_hidden_layers, p_window, p_min_count, p_workers, p_save_model=False):\n",
    "       \n",
    "    w2v_model = Word2Vec(p_documents, size=p_num_hidden_layers, window=p_window, min_count=p_min_count, workers=p_workers)\n",
    "    if p_save_model:\n",
    "        w2v_model.wv.save_word2vec_format(fname=\"D:/dergipark_word2vec.model\", fvocab=\"D:/dergipark_word2vec.vocab\")\n",
    "\n",
    "    print(\"Sözlükteki kelime sayısı: \" + str(len(w2v_model.wv.vocab)))\n",
    "    vocabulary_tuple = w2v_model.vocab.items() \n",
    "    vocabulary = list()\n",
    "    for (a, _) in vocabulary_tuple:\n",
    "        vocabulary.append(a)\n",
    "        \n",
    "    word_vectors = [] \n",
    "    for word in vocabulary: \n",
    "        word_vectors.append(w2v_model[word])\n",
    "        \n",
    "    return (word_vectors, vocabulary, w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = getDocumentList(\"C://icerik_dergipark.csv\")\n",
    "word_vectors, vocabulary, w2v_model = train_word2vec(documents, 300, 8, 5, 4)\n",
    "print(w2v_model.most_similar(positive=['deep', 'learning'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the pre-saved model\n",
    "#from gensim.models import KeyedVectors\n",
    "#model = KeyedVectors.load_word2vec_format(\"D:/dergipark_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(documents, low, high):\n",
    "    \"\"\"\n",
    "        Keyword extractin based on document frequency values\n",
    "        Keywords are words with frequency values between low and high  \n",
    "    \"\"\"\n",
    "    (a, keywords) = tf_idf_vectorizer(documents, high, low, (0,1)) \n",
    "    print \"The number of keywords are \", len(keywords)\n",
    "    return keywords\n",
    "\n",
    "def enrich_text(documents, w2v_train_set, keywords,  n_related_terms, w2v_trained):\n",
    "    \n",
    "    \"\"\"\n",
    "       Expanding text with related words in order to explicitly separate it \n",
    "       from other documents.\n",
    "       if text contains a sentence with keyword then it will be expanded \n",
    "       by adding related words to that keyword to the sentence \n",
    "    \"\"\"\n",
    "    \n",
    "    if w2v_trained == None: \n",
    "        (vectors, vocab, model) = train_word2vec(w2v_train_set, 300, 6, 1)\n",
    "    else: \n",
    "        model = w2v_trained \n",
    "        \n",
    "    # Getting related terms to the keywords \n",
    "    related_terms = []\n",
    "    related_terms_dict = {}\n",
    "    expanded_keywords = keywords \n",
    "    \n",
    "    for item in expanded_keywords:\n",
    "        a = getSimilarTerms(item, n_related_terms,  model)\n",
    "        for i in a:\n",
    "            string = \"\"\n",
    "            try: \n",
    "                string = string + \" \" + i\n",
    "            except UnicodeDecodeError:\n",
    "                print \"cannot decode the bytsstring %s\", i\n",
    "        if string != \"\":\n",
    "            related_terms.append(string)\n",
    "            related_terms_dict[item] = string\n",
    "    \n",
    "    documents_split = []\n",
    "    for doc in documents:\n",
    "    \tdocuments_split.append(doc.split()) \n",
    "    \n",
    "    # text enrichment  \n",
    "    for doc in documents_split: \n",
    "        for i in range(len(doc)): \n",
    "            if doc[i] in expanded_keywords:\n",
    "                doc[i] = related_terms_dict[doc[i]]\n",
    "\n",
    "    final = [] \n",
    "    for i in documents_split: \n",
    "        final.append(\" \".join(i))\n",
    "        \n",
    "    return (final, related_terms)\n",
    "\n",
    "\n",
    "def getSimilarTerms(term, N,  model):\n",
    "    a = []  \n",
    "    try: \n",
    "        a = model.most_similar(term, topn=N)\n",
    "    except: \n",
    "        print \"The word \", term, \" does not exists!\"\n",
    "    result = [term] * N  \n",
    "\n",
    "    # print a\n",
    "    K = N \n",
    "    for (i, _) in a:\n",
    "        K = K -1\n",
    "        result = result + [i] * K \n",
    "    return result \n",
    "\n",
    "\n",
    "## LEMMATIZATION \n",
    "def lemmatize(doc):\n",
    "    \"\"\"\n",
    "    Lemmatization\n",
    "    \"\"\"\n",
    "    lemm = {} \n",
    "    with open('lem.txt') as f:\n",
    "        for line in f:\n",
    "            a =  re.sub(' +',' ',line).split()\n",
    "            lemm[a[1]] = a[0]  \n",
    "    error = 0\n",
    "    success = 0\n",
    "    docl = []\n",
    "    for i in doc.split():\n",
    "        try:\n",
    "            success += 1\n",
    "            docl.append(lemm[i])\n",
    "        except KeyError:\n",
    "            error += 1\n",
    "            docl.append(i)\n",
    "    return \" \".join(docl),success, error\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "USAGE: \n",
    "model =Word2Vec.load('model')\n",
    "query = \"the query\"\n",
    "documents = readFile_(file_name) \n",
    "num_results = 200\n",
    "tfidf_search_result = tfidf_search(query, documents, num_results)\n",
    "query_expansion_search = query_expansion_search(query, documents, num_results, isEnriched, w2vmodel):\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Similarity of Documents By WordEmbeddings</b>\n",
    "\n",
    "Pretrained model is loaded. Then the cosine similarity matrices are calculated on the model. Then the query is converted to the bag-of-words representation. Finally the similarity of the query to each document are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\dev\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec, WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SoftCosineSimilarity, SparseTermSimilarityMatrix\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"C://icerik_dergipark.csv\", sep=\";\", encoding=\"utf8\")\n",
    "documents = data['icerik'].tolist()\n",
    "doc_lst = [] \n",
    "for doc in documents:\n",
    "    doc_lst.append(doc.split())\n",
    "\n",
    "\n",
    "#model = Word2Vec(common_texts, size=20, min_count=1)  # train word-vectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\"D:/dergipark_word2vec.model\")\n",
    "print(\"1\")\n",
    "termsim_index = WordEmbeddingSimilarityIndex(model.wv)\n",
    "print(\"2\")\n",
    "dictionary = Dictionary(doc_lst)\n",
    "print(\"3\")\n",
    "bow_corpus = [dictionary.doc2bow(document) for document in doc_lst]\n",
    "print(\"4\")\n",
    "similarity_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary)  # construct similarity matrix\n",
    "print(\"5\")\n",
    "docsim_index = SoftCosineSimilarity(bow_corpus, similarity_matrix, num_best=10)\n",
    "print(\"6\")\n",
    "\n",
    "query = 'makine öğrenmesi deep learning expert finding'.split()  # make a query\n",
    "sims = docsim_index[dictionary.doc2bow(query)]  # calculate similarity of query to each doc from bow_corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
